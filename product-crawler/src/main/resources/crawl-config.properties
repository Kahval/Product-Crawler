# crawl-config.properties
# Crawler config file
# All crawlers will have the same config.

# The folder which will be used by crawler for storing the intermediate
# crawl data. The content of this folder should not be modified manually.
CRAWL_STORAGE=./crawlerdata/root

# If this feature is enabled, you would be able to resume a previously
# stopped/crashed crawl. However, it makes crawling slightly slower
RESUMABLE=false

# Maximum depth of crawling For unlimited depth this parameter should be set to -1
MAX_DEPTH=-1

# Maximum number of pages to fetch For unlimited number of pages, this parameter should be set to -1
MAX_PAGES=-1

# user-agent string that is used for representing your crawler to web servers
USERAGENT=crawler4j

# Politeness delay in milliseconds (delay between sending two requests to the same host)
POLITENESS_DELAY=1000

# Should we also crawl https pages?
INCLUDE_HTTPS=true

# Should we fetch binary content such as images, audio, ...?
INCLUDE_BINARY_CONTENT=false

# Should we process binary content such as image, audio, ... using TIKA?
PROCESS_BINARY_CONTENT=false

# Maximum Connections per host
MAX_CONNECTION_PER_HOST=100

# Maximum total connections
MAX_TOTAL_CONNECTIONS=100

# Socket timeout in milliseconds
SOCKET_TIMEOUT=20000

# Connection timeout in milliseconds
CONNECTION_TIMEOUT=30000

# Max number of outgoing links which are processed from a page
MAX_OUTGOING_LINKS=5000

# Max allowed size of a page. Pages larger than this size will not be fetched.
MAX_DOWNLOAD_SIZE=1048576

# Should we follow redirects?
FOLLOW_REDIRECTS=true

# Should the TLD list be updated automatically on each run? Alternatively,
# it can be loaded from the embedded tld-names.zip file that was obtained from
# https://publicsuffix.org/list/effective_tld_names.dat
ONLINE_TLD_LIST_UPDATE=false

# Should the crawler stop running when the queue is empty?
SHUTDOWN_ON_EMPTY_QUEUE=true

# Wait this long (seconds) before checking the status of the worker threads.
THREAD_MONITORING_DELAY=10

# Wait this long (seconds) to verify the craweler threads are finished working.
THREAD_SHUTDOWN_DELAY=10

# Wait this long in seconds before launching cleanup.
CLEANUP_DELAY=10

# If crawler should run behind a proxy, this parameter can be used for
# specifying the proxy host.
PROXY_HOST=

# If crawler should run behind a proxy, this parameter can be used for
# specifying the proxy port.
PROXY_PORT=80

# If crawler should run behind a proxy and user/pass is needed for
# authentication in proxy, this parameter can be used for specifying the
# username.
PROXY_USERNAME=

# If crawler should run behind a proxy and user/pass is needed for
# authentication in proxy, this parameter can be used for specifying the
# password.
PROXY_PASSWORD=



































